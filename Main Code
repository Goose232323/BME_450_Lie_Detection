# Micro-expression Lie Detection using Dual-Stream Transformer (Visual + Motion)

import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import random_split
from torchvision import transforms, models
from PIL import Image
import cv2
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# -----------------------------
# Config
# -----------------------------

PARENT_DIR = "/kaggle/input/micro-expression-dataset-for-lie-detection"

PARTICIPANT_NAMES = [
    "Atul", "Dishant", "DrPrashant", "Harsha", "Mansi", "Mansvi", "Sailja"
]

COMMON_QUESTIONS_TRAIN = [
    "What is your Favorite Color",
    "What is your Favorite Food",
    "What is your Hobby",
    "You are a Morning person or Night Owl"
]

COMMON_QUESTIONS_TEST = [
    "What is your Name",
    "What is your Profession"
]

PARTICIPANTS = {
    name: COMMON_QUESTIONS_TRAIN for name in PARTICIPANT_NAMES
}

IMG_SIZE = 224
SEQ_LEN = 10
BATCH_SIZE = 1
NUM_WORKERS = 2
EPOCHS = 3

# -----------------------------
# Optical Flow Calculation
# -----------------------------

def compute_optical_flow(prev_img, next_img):
    prev_gray = cv2.cvtColor(np.array(prev_img), cv2.COLOR_RGB2GRAY)
    next_gray = cv2.cvtColor(np.array(next_img), cv2.COLOR_RGB2GRAY)
    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
    flow = np.clip(flow, -20, 20) / 20.0  # normalize
    return flow  # shape: (H, W, 2)

# -----------------------------
# Dataset Class
# -----------------------------

class DualStreamDataset(Dataset):
    def __init__(self, participant_name, question_list, root_dir, transform=None, seq_len=SEQ_LEN, mode="Train"):
        self.samples = []
        self.transform = transform
        self.seq_len = seq_len

        if mode == "Train":
            base_path = os.path.join(root_dir, "Train/Train")
        else:
            base_path = os.path.join(root_dir, "Test/Test")

        for label_name, label in zip(["Lie", "Truth"], [0, 1]):
            for question in question_list:
                path = os.path.join(base_path, label_name, participant_name, question)
                if not os.path.exists(path):
                    continue
                frames = sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith(".png")])
                if len(frames) < 2:
                    continue
                self.samples.append((frames, label))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        frame_paths, label = self.samples[idx]
        frames = []
        flows = []

        for i in range(min(len(frame_paths) - 1, self.seq_len)):
            img1 = Image.open(frame_paths[i]).convert("RGB")
            img2 = Image.open(frame_paths[i+1]).convert("RGB")
            if self.transform:
                img1 = self.transform(img1)
                img2 = self.transform(img2)
            flow = compute_optical_flow(transforms.ToPILImage()(img1), transforms.ToPILImage()(img2))
            flow_tensor = torch.from_numpy(flow).permute(2, 0, 1).float()
            if flow_tensor.shape[0] == 2:
                pad = torch.zeros((1, *flow_tensor.shape[1:]))
                flow_tensor = torch.cat([flow_tensor, pad], dim=0)
            frames.append(img1)
            flows.append(flow_tensor)

        while len(frames) < self.seq_len:
            frames.append(frames[-1])
            flows.append(flows[-1])

        return torch.stack(frames), torch.stack(flows), label

# -----------------------------
# Transforms
# -----------------------------

transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
])

# -----------------------------
# Model Definition
# -----------------------------

class SimpleTransformer(nn.Module):
    def __init__(self, in_dim=512, seq_len=SEQ_LEN):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=in_dim, nhead=8, dim_feedforward=1024, dropout=0.1)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.cls_head = nn.Linear(in_dim, 2)

    def forward(self, x):
        x = self.transformer(x)
        x = x.mean(dim=1)
        return self.cls_head(x)

class DualStreamModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.rgb_backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        self.flow_backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        self.rgb_backbone.fc = nn.Identity()
        self.flow_backbone.fc = nn.Identity()
        self.rgb_proj = nn.Linear(512, 256)
        self.flow_proj = nn.Linear(512, 256)
        self.temporal = SimpleTransformer(in_dim=512)

    def forward(self, rgb_seq, flow_seq):
        B, T, C, H, W = rgb_seq.shape
        rgb_feats = []
        flow_feats = []
        for t in range(T):
            rgb_feat = self.rgb_proj(self.rgb_backbone(rgb_seq[:, t]))
            flow_feat = self.flow_proj(self.flow_backbone(flow_seq[:, t]))
            rgb_feats.append(rgb_feat)
            flow_feats.append(flow_feat)
        feats = [torch.cat([r, f], dim=1) for r, f in zip(rgb_feats, flow_feats)]
        seq = torch.stack(feats, dim=1)  # (B, T, 512)
        return self.temporal(seq)
# -----------------------------
# Training, Evaluation, and Testing
# -----------------------------

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for participant in PARTICIPANT_NAMES:
    print(f"Training for {participant}")
    dataset = DualStreamDataset(participant, COMMON_QUESTIONS_TRAIN, PARENT_DIR, transform=transform)
    total_len = len(dataset)
    val_len = max(1, total_len // 5)
    train_len = total_len - val_len
    train_set, val_set = random_split(dataset, [train_len, val_len])

    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

    model = DualStreamModel().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model = None

    for epoch in range(EPOCHS):
        model.train()
        total_train_loss = 0
        for rgb_seq, flow_seq, label in train_loader:
            rgb_seq, flow_seq, label = rgb_seq.to(device), flow_seq.to(device), label.to(device)
            out = model(rgb_seq, flow_seq)
            loss = criterion(out, label)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for rgb_seq, flow_seq, label in val_loader:
                rgb_seq, flow_seq, label = rgb_seq.to(device), flow_seq.to(device), label.to(device)
                out = model(rgb_seq, flow_seq)
                loss = criterion(out, label)
                total_val_loss += loss.item()
        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model = model.state_dict()

        print(f"{participant} | Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}")

    # Visualization
    plt.figure(figsize=(8, 4))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.title(f"Training Progress - {participant}")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Load best model and test
    model.load_state_dict(best_model)
    model.eval()
    test_dataset = DualStreamDataset(participant, COMMON_QUESTIONS_TEST, PARENT_DIR, transform=transform, mode="Test")
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

    y_true = []
    y_pred = []
    with torch.no_grad():
        for rgb_seq, flow_seq, label in test_loader:
            rgb_seq, flow_seq = rgb_seq.to(device), flow_seq.to(device)
            out = model(rgb_seq, flow_seq)
            pred = torch.argmax(out, dim=1).cpu().item()
            y_pred.append(pred)
            y_true.append(label.item())

    print(f"Test Results for {participant}:")
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Lie", "Truth"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix - {participant}")
    plt.show()
